{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fce6449b-a293-494f-a0b5-ffc6b76a55be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /local_disk0/.ephemeral_nfs/envs/pythonEnv-98e93b9a-00d6-4a23-8025-ceffd4ca15cf/lib/python3.12/site-packages (3.1.5)\nRequirement already satisfied: et-xmlfile in /local_disk0/.ephemeral_nfs/envs/pythonEnv-98e93b9a-00d6-4a23-8025-ceffd4ca15cf/lib/python3.12/site-packages (from openpyxl) (2.0.0)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: regex in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.12/site-packages (2024.11.6)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting tqdm\n  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/57.7 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.7/57.7 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/78.5 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: tqdm\nSuccessfully installed tqdm-4.67.1\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install openpyxl \n",
    "%pip install regex\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2949a85d-7c23-46e7-acaf-45c2e48f1146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Any\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d73a5516-280d-4c27-9bf1-643c876be050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Nouveau regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354fee52-2b39-400e-b3d1-1731eb65e5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AuthConfig:\n",
    "    base_url: str\n",
    "    client_id: str\n",
    "    client_secret: str\n",
    "    scope: str\n",
    "\n",
    "class AuthClient:\n",
    "    def __init__(self, config: AuthConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_token(self) -> Dict[str, str]:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.config.base_url}/as/token.oauth2\",\n",
    "                headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n",
    "                data={\n",
    "                    \"grant_type\": \"client_credentials\",\n",
    "                    \"client_id\": self.config.client_id,\n",
    "                    \"client_secret\": self.config.client_secret,\n",
    "                    \"scope\": self.config.scope\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Échec de l'authentification: {str(e)}\")\n",
    "\n",
    "def get_auth_token() -> str:\n",
    "    config = AuthConfig(\n",
    "        base_url=\"https://onelogin.stg.axa.com\",\n",
    "        client_id=\"VJKkswZb\",\n",
    "        client_secret=\"yG3pl6nkYhvJlQVD1y7xbZBl692YRytdpUJeODKg5BAutoEgxhkqzXLFYGqH5zlp\",\n",
    "        scope=\"urn:grp:chatgpt\"\n",
    "    )\n",
    "    client = AuthClient(config)\n",
    "    result = client.get_token()\n",
    "    return result.get(\"access_token\", \"\")\n",
    "\n",
    "class SecureGPTRegexImprover:\n",
    "    def __init__(self, \n",
    "                token: str,\n",
    "                temperature: float = 0.05,\n",
    "                base_url: str = \"https://api-pp.se.axa-go.axa.com/ago-m365-securegpt-bapi-v1-vrs\",\n",
    "                model: str = \"gpt-4o-2024-08-06\"):\n",
    "        self.token = token\n",
    "        self.temperature = temperature\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.costs = {\n",
    "            'input_token': 2.50/1_000_000,\n",
    "            'output_token': 10.00/1_000_000\n",
    "        }\n",
    "\n",
    "    def calculate_cost(self, response: Dict) -> Dict[str, float]:\n",
    "        metrics = {'token_cost': 0, 'total_cost': 0}\n",
    "\n",
    "        if 'usage' in response:\n",
    "            input_tokens = response['usage'].get('prompt_tokens', 0)\n",
    "            output_tokens = response['usage'].get('completion_tokens', 0)\n",
    "            metrics['token_cost'] = (input_tokens * self.costs['input_token'] + \n",
    "                                   output_tokens * self.costs['output_token'])\n",
    "            metrics['total_cost'] = metrics['token_cost']\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def chat(self, messages: List[Dict[str, Any]], **kwargs) -> Dict:\n",
    "        url = f\"{self.base_url}/deployments/{self.model}/chat/completions\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.token}\"\n",
    "        }\n",
    "        data = {\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", 4000)\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            api_response = response.json()\n",
    "            cost_metrics = self.calculate_cost(api_response)\n",
    "            api_response['cost_metrics'] = cost_metrics\n",
    "            return api_response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Détails de l'erreur API: {response.text if 'response' in locals() else 'Pas de réponse'}\")\n",
    "            raise Exception(f\"Échec de l'appel API: {str(e)}\")\n",
    "\n",
    "    def improve_regex_batch(self, \n",
    "                           batch_data: List[Dict[str, str]], \n",
    "                           system_prompt: str,\n",
    "                           additional_user_info: str = \"\") -> List[Dict]:\n",
    "        formatted_batch = json.dumps(batch_data, ensure_ascii=False, indent=2)\n",
    "        user_content = f\"\"\"\n",
    "Voici un lot de règles regex à améliorer:\n",
    "\n",
    "{formatted_batch}\n",
    "\n",
    "{additional_user_info}\n",
    "\n",
    "Pour chaque regex, suggérez une version améliorée qui prend en compte:\n",
    "- Variations de casse (majuscules/minuscules)\n",
    "- Variations grammaticales (singulier/pluriel, masculin/féminin)\n",
    "- Formats alternatifs (pour les dates, nombres, etc.)\n",
    "- Robustesse face aux erreurs courantes\n",
    "\n",
    "Renvoyez-moi un JSON avec les mêmes clés que l'entrée, mais avec les colonnes \"regex_rule_amelio\" et \"amelio_explanation\" remplies.\n",
    "\"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "        \n",
    "        max_retries = 5\n",
    "        retry_delay = 15\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.chat(messages)\n",
    "                response_content = response['choices'][0]['message']['content']\n",
    "                \n",
    "                clean_content = response_content.strip()\n",
    "                if clean_content.startswith(\"```json\"):\n",
    "                    clean_content = clean_content[7:]\n",
    "                if clean_content.endswith(\"```\"):\n",
    "                    clean_content = clean_content[:-3]\n",
    "                clean_content = clean_content.strip()\n",
    "                \n",
    "                improved_batch = json.loads(clean_content)\n",
    "                \n",
    "                for item in improved_batch:\n",
    "                    item['cost_metrics'] = response.get('cost_metrics', {})\n",
    "                \n",
    "                return improved_batch\n",
    "            \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Erreur lors de l'amélioration du lot (tentative {attempt+1}/{max_retries}): {str(e)}\")\n",
    "                    print(f\"Nouvelle tentative dans {retry_delay} secondes...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"Échec de l'amélioration du lot après {max_retries} tentatives: {str(e)}\")\n",
    "                    for item in batch_data:\n",
    "                        item['regex_rule_amelio'] = \"\"\n",
    "                        item['amelio_explanation'] = \"\"\n",
    "                        item['cost_metrics'] = {'ERROR': str(e)}\n",
    "                    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37b03fe8-c259-482b-abcb-0f921a6211a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_regex_files(excel_file: str) -> tuple:\n",
    "    try:\n",
    "        regex_mapping = pd.read_excel(excel_file, sheet_name='regex_mapping')\n",
    "        regex_definition = pd.read_excel(excel_file, sheet_name='regex_definition')\n",
    "        \n",
    "        print(f\"Onglet regex_mapping chargé avec {len(regex_mapping)} lignes\")\n",
    "        print(f\"Onglet regex_definition chargé avec {len(regex_definition)} lignes\")\n",
    "        \n",
    "        return regex_mapping, regex_definition\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erreur lors de la lecture du fichier Excel: {str(e)}\")\n",
    "\n",
    "def join_regex_dataframes(regex_mapping: pd.DataFrame, regex_definition: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        if 'regex_id' not in regex_mapping.columns:\n",
    "            raise ValueError(\"La colonne 'regex_id' est manquante dans regex_mapping\")\n",
    "        if 'regex_id' not in regex_definition.columns:\n",
    "            raise ValueError(\"La colonne 'regex_id' est manquante dans regex_definition\")\n",
    "            \n",
    "        joined_df = pd.merge(\n",
    "            regex_mapping,\n",
    "            regex_definition,\n",
    "            on='regex_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        print(f\"Jointure effectuée avec succès, résultat: {len(joined_df)} lignes\")\n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erreur lors de la jointure des dataframes: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5511de-d80b-42ab-9a08-0d78b686e039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Onglet regex_mapping transformé avec 35 lignes\nOnglet regex_definition chargé avec 35 lignes\nJointure effectuée avec succès, résultat: 35 lignes\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats intermédiaires sauvegardés dans regex_improved_results_temp_0.xlsx\nCoût cumulatif: 0.053963 $\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  14%|█▍        | 1/7 [00:11<01:08, 11.45s/it]\rTraitement des lots:  29%|██▊       | 2/7 [00:21<00:54, 10.80s/it]\rTraitement des lots:  43%|████▎     | 3/7 [01:05<01:43, 25.91s/it]\rTraitement des lots:  57%|█████▋    | 4/7 [01:34<01:21, 27.14s/it]\rTraitement des lots:  71%|███████▏  | 5/7 [02:09<00:59, 29.90s/it]\rTraitement des lots:  86%|████████▌ | 6/7 [02:29<00:26, 26.46s/it]\rTraitement des lots: 100%|██████████| 7/7 [03:04<00:00, 29.23s/it]\rTraitement des lots: 100%|██████████| 7/7 [03:04<00:00, 26.32s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé. Résultats sauvegardés dans regex_improved_results.xlsx\nCoût total: 0.377563 $\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Any\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class AuthConfig:\n",
    "    base_url: str\n",
    "    client_id: str\n",
    "    client_secret: str\n",
    "    scope: str\n",
    "\n",
    "class AuthClient:\n",
    "    def __init__(self, config: AuthConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_token(self) -> Dict[str, str]:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.config.base_url}/as/token.oauth2\",\n",
    "                headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n",
    "                data={\n",
    "                    \"grant_type\": \"client_credentials\",\n",
    "                    \"client_id\": self.config.client_id,\n",
    "                    \"client_secret\": self.config.client_secret,\n",
    "                    \"scope\": self.config.scope\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Échec de l'authentification: {str(e)}\")\n",
    "\n",
    "def get_auth_token() -> str:\n",
    "    config = AuthConfig(\n",
    "        base_url=\"https://onelogin.stg.axa.com\",\n",
    "        client_id=\"VJKkswZb\",\n",
    "        client_secret=\"yG3pl6nkYhvJlQVD1y7xbZBl692YRytdpUJeODKg5BAutoEgxhkqzXLFYGqH5zlp\",\n",
    "        scope=\"urn:grp:chatgpt\"\n",
    "    )\n",
    "    client = AuthClient(config)\n",
    "    result = client.get_token()\n",
    "    return result.get(\"access_token\", \"\")\n",
    "\n",
    "class SecureGPTRegexImprover:\n",
    "    def __init__(self, \n",
    "                token: str,\n",
    "                temperature: float = 0.05,\n",
    "                base_url: str = \"https://api-pp.se.axa-go.axa.com/ago-m365-securegpt-bapi-v1-vrs\",\n",
    "                model: str = \"gpt-4o-2024-08-06\"):\n",
    "        self.token = token\n",
    "        self.temperature = temperature\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.costs = {\n",
    "            'input_token': 2.50/1_000_000,\n",
    "            'output_token': 10.00/1_000_000\n",
    "        }\n",
    "\n",
    "    def calculate_cost(self, response: Dict) -> Dict[str, float]:\n",
    "        metrics = {'token_cost': 0, 'total_cost': 0}\n",
    "\n",
    "        if 'usage' in response:\n",
    "            input_tokens = response['usage'].get('prompt_tokens', 0)\n",
    "            output_tokens = response['usage'].get('completion_tokens', 0)\n",
    "            metrics['token_cost'] = (input_tokens * self.costs['input_token'] + \n",
    "                                   output_tokens * self.costs['output_token'])\n",
    "            metrics['total_cost'] = metrics['token_cost']\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def chat(self, messages: List[Dict[str, Any]], **kwargs) -> Dict:\n",
    "        url = f\"{self.base_url}/deployments/{self.model}/chat/completions\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.token}\"\n",
    "        }\n",
    "        data = {\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", 4000)\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            api_response = response.json()\n",
    "            cost_metrics = self.calculate_cost(api_response)\n",
    "            api_response['cost_metrics'] = cost_metrics\n",
    "            return api_response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Détails de l'erreur API: {response.text if 'response' in locals() else 'Pas de réponse'}\")\n",
    "            raise Exception(f\"Échec de l'appel API: {str(e)}\")\n",
    "\n",
    "    def improve_regex_batch(self, \n",
    "                           batch_data: List[Dict[str, str]], \n",
    "                           system_prompt: str,\n",
    "                           additional_user_info: str = \"\") -> List[Dict]:\n",
    "        formatted_batch = json.dumps(batch_data, ensure_ascii=False, indent=2)\n",
    "        user_content = f\"\"\"\n",
    "Voici un lot de règles regex à améliorer:\n",
    "\n",
    "{formatted_batch}\n",
    "\n",
    "{additional_user_info}\n",
    "\n",
    "Pour chaque regex, suggérez une version améliorée qui prend en compte:\n",
    "- Variations de casse (majuscules/minuscules)\n",
    "- Variations grammaticales (singulier/pluriel, masculin/féminin)\n",
    "- Formats alternatifs (pour les dates, nombres, etc.)\n",
    "- Robustesse face aux erreurs courantes\n",
    "\n",
    "Renvoyez-moi un JSON avec les mêmes clés que l'entrée, mais avec les colonnes \"regex_rule_amelio\" et \"amelio_explanation\" remplies.\n",
    "\"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "        \n",
    "        max_retries = 5\n",
    "        retry_delay = 15\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.chat(messages)\n",
    "                response_content = response['choices'][0]['message']['content']\n",
    "                \n",
    "                clean_content = response_content.strip()\n",
    "                if clean_content.startswith(\"```json\"):\n",
    "                    clean_content = clean_content[7:]\n",
    "                if clean_content.endswith(\"```\"):\n",
    "                    clean_content = clean_content[:-3]\n",
    "                clean_content = clean_content.strip()\n",
    "                \n",
    "                improved_batch = json.loads(clean_content)\n",
    "                \n",
    "                for item in improved_batch:\n",
    "                    item['cost_metrics'] = response.get('cost_metrics', {})\n",
    "                \n",
    "                return improved_batch\n",
    "            \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Erreur lors de l'amélioration du lot (tentative {attempt+1}/{max_retries}): {str(e)}\")\n",
    "                    print(f\"Nouvelle tentative dans {retry_delay} secondes...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"Échec de l'amélioration du lot après {max_retries} tentatives: {str(e)}\")\n",
    "                    # Si l'API échoue après toutes les tentatives, indiquer clairement l'échec dans les colonnes résultats\n",
    "                    for item in batch_data:\n",
    "                        item['regex_rule_amelio'] = \"ERREUR API: Échec après 5 tentatives\"\n",
    "                        item['amelio_explanation'] = f\"Erreur: {str(e)}\"\n",
    "                        item['cost_metrics'] = {'ERROR': str(e)}\n",
    "                    return batch_data\n",
    "\n",
    "def read_regex_files(excel_file: str) -> tuple:\n",
    "    try:\n",
    "        # Modification pour lire le format spécifique de l'onglet regex_mapping\n",
    "        # Dans ce format, la première ligne contient les noms de colonnes et la seconde ligne contient les IDs\n",
    "        raw_mapping = pd.read_excel(excel_file, sheet_name='regex_mapping', header=None)\n",
    "        \n",
    "        # La première ligne contient les noms de colonnes\n",
    "        column_names = raw_mapping.iloc[0].tolist()\n",
    "        \n",
    "        # La deuxième ligne contient les IDs\n",
    "        regex_ids = raw_mapping.iloc[1].tolist()\n",
    "        \n",
    "        # Créer un DataFrame transformé au format attendu\n",
    "        mapping_data = []\n",
    "        for i, (column_name, regex_id) in enumerate(zip(column_names, regex_ids)):\n",
    "            if pd.notna(column_name) and pd.notna(regex_id):\n",
    "                mapping_data.append({\n",
    "                    'column_name': column_name,\n",
    "                    'regex_id': regex_id\n",
    "                })\n",
    "        \n",
    "        regex_mapping = pd.DataFrame(mapping_data)\n",
    "        \n",
    "        # Lire l'onglet regex_definition normalement\n",
    "        regex_definition = pd.read_excel(excel_file, sheet_name='regex_definition')\n",
    "        \n",
    "        print(f\"Onglet regex_mapping transformé avec {len(regex_mapping)} lignes\")\n",
    "        print(f\"Onglet regex_definition chargé avec {len(regex_definition)} lignes\")\n",
    "        \n",
    "        return regex_mapping, regex_definition\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erreur lors de la lecture du fichier Excel: {str(e)}\")\n",
    "\n",
    "def join_regex_dataframes(regex_mapping: pd.DataFrame, regex_definition: pd.DataFrame) -> pd.DataFrame:\n",
    "    try:\n",
    "        if 'regex_id' not in regex_mapping.columns:\n",
    "            raise ValueError(\"La colonne 'regex_id' est manquante dans regex_mapping\")\n",
    "        if 'regex_id' not in regex_definition.columns:\n",
    "            raise ValueError(\"La colonne 'regex_id' est manquante dans regex_definition\")\n",
    "            \n",
    "        joined_df = pd.merge(\n",
    "            regex_mapping,\n",
    "            regex_definition,\n",
    "            on='regex_id',\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        print(f\"Jointure effectuée avec succès, résultat: {len(joined_df)} lignes\")\n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erreur lors de la jointure des dataframes: {str(e)}\")\n",
    "\n",
    "def process_regex_improvement(\n",
    "    excel_file: str,\n",
    "    output_file: str,\n",
    "    system_prompt: str,\n",
    "    additional_user_info: str = \"\",\n",
    "    batch_size: int = 5,\n",
    "    save_interval: int = 10\n",
    ") -> None:\n",
    "    regex_mapping, regex_definition = read_regex_files(excel_file)\n",
    "    \n",
    "    joined_df = join_regex_dataframes(regex_mapping, regex_definition)\n",
    "    \n",
    "    improver = SecureGPTRegexImprover(\n",
    "        token=get_auth_token(),\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    data_to_improve = []\n",
    "    for _, row in joined_df.iterrows():\n",
    "        item = {\n",
    "            'regex_id': row['regex_id'],\n",
    "            'column_name': row['column_name'] if 'column_name' in row else \"\",\n",
    "            'regex_rule': row['regex_rule'] if 'regex_rule' in row else \"\",\n",
    "            'regex_description': row['regex_description'] if 'regex_description' in row else \"\",\n",
    "            'regex_rule_amelio': \"\",\n",
    "            'amelio_explanation': \"\"\n",
    "        }\n",
    "        data_to_improve.append(item)\n",
    "    \n",
    "    results = []\n",
    "    total_cost = 0\n",
    "    \n",
    "    for i in tqdm(range(0, len(data_to_improve), batch_size), desc=\"Traitement des lots\"):\n",
    "        batch = data_to_improve[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            improved_batch = improver.improve_regex_batch(\n",
    "                batch, \n",
    "                system_prompt, \n",
    "                additional_user_info\n",
    "            )\n",
    "            \n",
    "            results.extend(improved_batch)\n",
    "            \n",
    "            for item in improved_batch:\n",
    "                if 'cost_metrics' in item and 'total_cost' in item['cost_metrics']:\n",
    "                    total_cost += item['cost_metrics']['total_cost']\n",
    "            \n",
    "            if (i // batch_size) % save_interval == 0:\n",
    "                temp_df = pd.DataFrame(results)\n",
    "                temp_output = f\"{os.path.splitext(output_file)[0]}_temp_{i}.xlsx\"\n",
    "                temp_df.to_excel(temp_output, index=False)\n",
    "                print(f\"Résultats intermédiaires sauvegardés dans {temp_output}\")\n",
    "                print(f\"Coût cumulatif: {total_cost:.6f} $\")\n",
    "            \n",
    "            time.sleep(1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du lot {i//batch_size + 1}: {str(e)}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(f\"Traitement terminé. Résultats sauvegardés dans {output_file}\")\n",
    "    print(f\"Coût total: {total_cost:.6f} $\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EXCEL_FILE = \"Regex_Health_Listado_asegurados.xlsx\"\n",
    "    OUTPUT_FILE = \"regex_improved_results.xlsx\"\n",
    "    \n",
    "    SYSTEM_PROMPT = \"\"\"Vous êtes un expert en expressions régulières (regex).\n",
    "Votre tâche est d'améliorer les regex pour les rendre plus robustes et complètes.\n",
    "\n",
    "Règles importantes:\n",
    "1. Assurez-vous que les regex améliorées couvrent plus de cas d'utilisation tout en restant précises.\n",
    "2. Prenez en compte les variations possibles comme la casse, le pluriel/singulier, les formats alternatifs, etc.\n",
    "3. Fournissez une explication claire et EN FRANÇAIS de vos modifications et des nouveaux cas couverts.\n",
    "4. Assurez-vous que les regex améliorées restent compatibles avec le format Python/PCRE.\n",
    "5. Testez mentalement vos regex sur divers exemples pour garantir leur robustesse.\n",
    "6. N'hésitez pas à utiliser des groupes non capturants (?:) pour améliorer la performance.\n",
    "\n",
    "Votre sortie doit être un JSON respectant strictement le format d'entrée, avec les champs \"regex_rule_amelio\" et \"amelio_explanation\" remplis. La colonne \"amelio_explanation\" DOIT être rédigée en français.\"\"\"\n",
    "    \n",
    "    ADDITIONAL_USER_INFO = \"\"\"Pour les regex concernant des noms propres, considérez les accents, tirets et apostrophes.\n",
    "Pour les dates, tenez compte des formats internationaux (DD/MM/YYYY, MM/DD/YYYY, YYYY-MM-DD).\n",
    "Pour les nombres, considérez les séparateurs différents selon les cultures (point, virgule).\n",
    "Pensez à rendre les regex robustes face aux espaces superflus en début/fin.\n",
    "Pour les formats d'identification, prenez en compte les variations de formatage (avec/sans espaces ou caractères spéciaux).\"\"\"\n",
    "    \n",
    "    process_regex_improvement(\n",
    "        excel_file=EXCEL_FILE,\n",
    "        output_file=OUTPUT_FILE,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        additional_user_info=ADDITIONAL_USER_INFO,\n",
    "        batch_size=5,\n",
    "        save_interval=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eedaaa0-db8f-4e9a-b33c-3cad170aa817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Correction déterministe avec fake ERROR llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a89f679-5b86-44f1-b3e0-3e671604a0ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Etape 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fbdf8bb-4139-43fc-a366-76bf76ad101f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes dans l'onglet 'ERROR': ['Columna', 'Linea', 'Valor Incorrecto', 'Descripcion', 'Correction']\nOnglet 'Macros Alta' chargé: 22 lignes, 35 colonnes\nOnglet 'ERROR' chargé: 113 lignes\nFichier regex chargé: 35 règles\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:   0%|          | 0/38 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"NUM_MIEMBROS_DE_LA_FAMILIA\",\n    \"Linea\": 0,\n    \"correction\": \"4\"\n  },\n  {\n    \"Columna\": \"NUM_MIEMBROS_DE_LA_FAMILIA\",\n    \"Linea\": 4,\n    \"correction\": \"3\"\n  },\n  {\n  ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:   3%|▎         | 1/38 [00:09<06:02,  9.81s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"NUM_MIEMBROS_DE_LA_FAMILIA\",\n    \"Linea\": 12,\n    \"correction\": \"3\"\n  },\n  {\n    \"Columna\": \"NUM_MIEMBROS_DE_LA_FAMILIA\",\n    \"Linea\": 13,\n    \"correction\": \"5\"\n  },\n  {\n...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:   5%|▌         | 2/38 [00:14<04:05,  6.82s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"NUM_MIEMBROS_DE_LA_FAMILIA\",\n    \"Linea\": 15,\n    \"correction\": \"3\"\n  },\n  {\n    \"Columna\": \"NUM_MIEMBROS_DE_LA_FAMILIA\",\n    \"Linea\": 16,\n    \"correction\": \"4\"\n  },\n  {\n...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:   8%|▊         | 3/38 [00:18<03:06,  5.33s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"NUM_MIEMBROS_DE_LA_FAMILIA\",\n    \"Linea\": 18,\n    \"correction\": \"3\"\n  },\n  {\n    \"Columna\": \"NUM_MIEMBROS_DE_LA_FAMILIA\",\n    \"Linea\": 19,\n    \"correction\": \"5\"\n  },\n  {\n...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  11%|█         | 4/38 [00:22<02:46,  4.90s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"NUM_MIEMBROS_DE_LA_FAMILIA\",\n    \"Linea\": 21,\n    \"correction\": \"4\"\n  },\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 0,\n    \"correction\": \"ES9121000418401234567890\"\n ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  13%|█▎        | 5/38 [00:26<02:30,  4.56s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 2,\n    \"correction\": \"ES9121000418450200051332\"\n  },\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 3,\n    \"correction\": \"ES76208000011830630...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  16%|█▌        | 6/38 [00:30<02:21,  4.42s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 5,\n    \"correction\": \"ES9121000418450200051332\"\n  },\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 6,\n    \"correction\": \"ES46203857794930200...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  18%|█▊        | 7/38 [00:35<02:20,  4.54s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 8,\n    \"correction\": \"ES9121000418450200051332\"\n  },\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 9,\n    \"correction\": \"ES76207700240031025...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  21%|██        | 8/38 [00:43<02:56,  5.87s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 11,\n    \"correction\": \"ES9121000418450200051332\"\n  },\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 12,\n    \"correction\": \"ES302080060144123...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  24%|██▎       | 9/38 [00:52<03:11,  6.59s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 14,\n    \"correction\": \"ES9121000418450200051332\"\n  },\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 15,\n    \"correction\": \"ES762077002400310...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  26%|██▋       | 10/38 [00:59<03:13,  6.91s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 17,\n    \"correction\": \"ES9121000418450200051332\"\n  },\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 18,\n    \"correction\": \"ES762077002400310...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  29%|██▉       | 11/38 [01:07<03:14,  7.19s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 20,\n    \"correction\": \"ES9121000418450200051332\"\n  },\n  {\n    \"Columna\": \"CODIGO_BANCO\",\n    \"Linea\": 21,\n    \"correction\": \"ES142080580411123...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  32%|███▏      | 12/38 [01:14<03:04,  7.10s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 1,\n    \"correction\": \"3\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 2,\n    \"correction\": \"7\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 3,\n    \"correcti...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  34%|███▍      | 13/38 [01:18<02:33,  6.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 4,\n    \"correction\": \"3\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 5,\n    \"correction\": \"7\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 6,\n    \"correcti...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  37%|███▋      | 14/38 [01:21<02:08,  5.34s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 7,\n    \"correction\": \"4\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 8,\n    \"correction\": \"7\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 9,\n    \"correcti...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  39%|███▉      | 15/38 [01:25<01:51,  4.86s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 10,\n    \"correction\": \"3\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 11,\n    \"correction\": \"7\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 12,\n    \"corre...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  42%|████▏     | 16/38 [01:32<02:00,  5.48s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 13,\n    \"correction\": \"5\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 14,\n    \"correction\": \"3\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 15,\n    \"corre...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  45%|████▍     | 17/38 [01:38<01:58,  5.65s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 16,\n    \"correction\": \"5\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 17,\n    \"correction\": \"3\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 18,\n    \"corre...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  47%|████▋     | 18/38 [01:46<02:07,  6.40s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 19,\n    \"correction\": \"3\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 20,\n    \"correction\": \"7\"\n  },\n  {\n    \"Columna\": \"DC\",\n    \"Linea\": 21,\n    \"corre...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  50%|█████     | 19/38 [01:50<01:47,  5.68s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"NIF_CIF_ASEGURADO\",\n    \"Linea\": 4,\n    \"correction\": \"X1234567L\"\n  },\n  {\n    \"Columna\": \"PRIMER_APELLIDO\",\n    \"Linea\": 5,\n    \"correction\": \"González\"\n  },\n  {\n    \"Co...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  53%|█████▎    | 20/38 [02:02<02:17,  7.64s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"SEGUNDO_APELLIDO\",\n    \"Linea\": 17,\n    \"correction\": \"González\"\n  },\n  {\n    \"Columna\": \"SEGUNDO_APELLIDO\",\n    \"Linea\": 20,\n    \"correction\": \"Fernández\"\n  },\n  {\n    \"...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  55%|█████▌    | 21/38 [02:10<02:08,  7.55s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 1,\n    \"correction\": \"soltero\"\n  },\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 2,\n    \"correction\": \"casado\"\n  },\n  {\n    \"Columna\": \"EST...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  58%|█████▊    | 22/38 [02:17<02:00,  7.53s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 5,\n    \"correction\": \"Casado\"\n  },\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 6,\n    \"correction\": \"Viudo\"\n  },\n  {\n    \"Columna\": \"ESTAD...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  61%|██████    | 23/38 [02:24<01:48,  7.25s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 8,\n    \"correction\": \"soltero\"\n  },\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 10,\n    \"correction\": \"casado\"\n  },\n  {\n    \"Columna\": \"ES...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  63%|██████▎   | 24/38 [02:30<01:37,  6.94s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 12,\n    \"correction\": \"soltero\"\n  },\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 13,\n    \"correction\": \"casado\"\n  },\n  {\n    \"Columna\": \"E...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  66%|██████▌   | 25/38 [02:36<01:26,  6.64s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 17,\n    \"correction\": \"Casado\"\n  },\n  {\n    \"Columna\": \"ESTADO_CIVIL\",\n    \"Linea\": 18,\n    \"correction\": \"Soltero\"\n  },\n  {\n    \"Columna\": \"E...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  68%|██████▊   | 26/38 [02:45<01:26,  7.22s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_POSTAL\",\n    \"Linea\": 0,\n    \"correction\": \"28001\"\n  },\n  {\n    \"Columna\": \"CODIGO_POSTAL\",\n    \"Linea\": 1,\n    \"correction\": \"08015\"\n  },\n  {\n    \"Columna\": \"CODI...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  71%|███████   | 27/38 [02:51<01:17,  7.06s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_POSTAL\",\n    \"Linea\": 3,\n    \"correction\": \"28013\"\n  },\n  {\n    \"Columna\": \"CODIGO_POSTAL\",\n    \"Linea\": 13,\n    \"correction\": \"08001\"\n  },\n  {\n    \"Columna\": \"COD...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  74%|███████▎  | 28/38 [02:55<01:00,  6.01s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"CODIGO_POSTAL\",\n    \"Linea\": 18,\n    \"correction\": \"28001\"\n  },\n  {\n    \"Columna\": \"CODIGO_POSTAL\",\n    \"Linea\": 19,\n    \"correction\": \"08002\"\n  },\n  {\n    \"Columna\": \"PO...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  76%|███████▋  | 29/38 [03:01<00:55,  6.16s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"POBLACION\",\n    \"Linea\": 5,\n    \"correction\": \"Madrid\"\n  },\n  {\n    \"Columna\": \"POBLACION\",\n    \"Linea\": 6,\n    \"correction\": \"Barcelona\"\n  },\n  {\n    \"Columna\": \"POBLACI...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  79%|███████▉  | 30/38 [03:06<00:44,  5.61s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 0,\n    \"correction\": \"15/09/2023\"\n  },\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 1,\n    \"correction\": \"01/01/2024\"\n  },\n  {\n    \"Columna...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  82%|████████▏ | 31/38 [03:12<00:41,  5.91s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 3,\n    \"correction\": \"15/07/2023\"\n  },\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 4,\n    \"correction\": \"01/09/2023\"\n  },\n  {\n    \"Columna...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  84%|████████▍ | 32/38 [03:21<00:41,  6.90s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 6,\n    \"correction\": \"15/03/2023\"\n  },\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 7,\n    \"correction\": \"01/01/2022\"\n  },\n  {\n    \"Columna...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  87%|████████▋ | 33/38 [03:29<00:35,  7.03s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 9,\n    \"correction\": \"15/03/2023\"\n  },\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 10,\n    \"correction\": \"28/07/2023\"\n  },\n  {\n    \"Column...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  89%|████████▉ | 34/38 [03:33<00:25,  6.25s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 12,\n    \"correction\": \"15/03/2023\"\n  },\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 13,\n    \"correction\": \"01/12/2022\"\n  },\n  {\n    \"Colum...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  92%|█████████▏| 35/38 [03:40<00:19,  6.47s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 15,\n    \"correction\": \"15/03/2023\"\n  },\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 16,\n    \"correction\": \"28/07/2022\"\n  },\n  {\n    \"Colum...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  95%|█████████▍| 36/38 [03:45<00:11,  5.86s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 18,\n    \"correction\": \"15/03/2023\"\n  },\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 19,\n    \"correction\": \"01/11/2022\"\n  },\n  {\n    \"Colum...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots:  97%|█████████▋| 37/38 [03:51<00:06,  6.13s/it]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Réponse brute de l'API (premiers 200 caractères): ```json\n[\n  {\n    \"Columna\": \"FECHA_EFECTO\",\n    \"Linea\": 21,\n    \"correction\": \"15/04/2023\"\n  },\n  {\n    \"Columna\": \"RAZON_SOCIAL\",\n    \"Linea\": 0,\n    \"correction\": \"Seguros Vida S.A.\"\n  }\n]\n```...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rTraitement des lots: 100%|██████████| 38/38 [03:57<00:00,  5.94s/it]\rTraitement des lots: 100%|██████████| 38/38 [03:57<00:00,  6.25s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Génération terminée. Coût total: 1.279940 $\nCorrections générées et sauvegardées dans ERROR_avec_corrections_llm.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "from typing import Dict, List, Optional, Any\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "@dataclass\n",
    "class AuthConfig:\n",
    "    base_url: str\n",
    "    client_id: str\n",
    "    client_secret: str\n",
    "    scope: str\n",
    "\n",
    "class AuthClient:\n",
    "    def __init__(self, config: AuthConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_token(self) -> Dict[str, str]:\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.config.base_url}/as/token.oauth2\",\n",
    "                headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n",
    "                data={\n",
    "                    \"grant_type\": \"client_credentials\",\n",
    "                    \"client_id\": self.config.client_id,\n",
    "                    \"client_secret\": self.config.client_secret,\n",
    "                    \"scope\": self.config.scope\n",
    "                }\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Échec de l'authentification: {str(e)}\")\n",
    "\n",
    "def get_auth_token() -> str:\n",
    "    config = AuthConfig(\n",
    "        base_url=\"https://onelogin.stg.axa.com\",\n",
    "        client_id=\"VJKkswZb\",\n",
    "        client_secret=\"yG3pl6nkYhvJlQVD1y7xbZBl692YRytdpUJeODKg5BAutoEgxhkqzXLFYGqH5zlp\",\n",
    "        scope=\"urn:grp:chatgpt\"\n",
    "    )\n",
    "    client = AuthClient(config)\n",
    "    result = client.get_token()\n",
    "    return result.get(\"access_token\", \"\")\n",
    "\n",
    "class SecureGPTCorrectionsGenerator:\n",
    "    def __init__(self, \n",
    "                token: str,\n",
    "                temperature: float = 0.7,\n",
    "                base_url: str = \"https://api-pp.se.axa-go.axa.com/ago-m365-securegpt-bapi-v1-vrs\",\n",
    "                model: str = \"gpt-4o-2024-08-06\"):\n",
    "        self.token = token\n",
    "        self.temperature = temperature\n",
    "        self.base_url = base_url\n",
    "        self.model = model\n",
    "        self.costs = {\n",
    "            'input_token': 2.50/1_000_000,\n",
    "            'output_token': 10.00/1_000_000\n",
    "        }\n",
    "\n",
    "    def calculate_cost(self, response: Dict) -> Dict[str, float]:\n",
    "        metrics = {'token_cost': 0, 'total_cost': 0}\n",
    "\n",
    "        if 'usage' in response:\n",
    "            input_tokens = response['usage'].get('prompt_tokens', 0)\n",
    "            output_tokens = response['usage'].get('completion_tokens', 0)\n",
    "            metrics['token_cost'] = (input_tokens * self.costs['input_token'] + \n",
    "                                   output_tokens * self.costs['output_token'])\n",
    "            metrics['total_cost'] = metrics['token_cost']\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def chat(self, messages: List[Dict[str, Any]], **kwargs) -> Dict:\n",
    "        url = f\"{self.base_url}/deployments/{self.model}/chat/completions\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.token}\"\n",
    "        }\n",
    "        data = {\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": self.temperature,\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", 4000)\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=data)\n",
    "            response.raise_for_status()\n",
    "            api_response = response.json()\n",
    "            cost_metrics = self.calculate_cost(api_response)\n",
    "            api_response['cost_metrics'] = cost_metrics\n",
    "            return api_response\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Détails de l'erreur API: {response.text if 'response' in locals() else 'Pas de réponse'}\")\n",
    "            raise Exception(f\"Échec de l'appel API: {str(e)}\")\n",
    "\n",
    "    def generer_correction_manuelle(self, erreur, regex_info):\n",
    "        \"\"\"\n",
    "        Génère une correction manuellement en cas d'échec du LLM\n",
    "        \"\"\"\n",
    "        colonne = erreur['Columna']\n",
    "        correction = \"\"\n",
    "\n",
    "        # Logique basique basée sur le type de colonne\n",
    "        if 'FECHA' in colonne:  # Date\n",
    "            correction = \"01/01/1980\"\n",
    "        elif 'NIF' in colonne or 'CIF' in colonne:  # Documents d'identité\n",
    "            correction = \"A12345678\"\n",
    "        elif 'NOMBRE' in colonne or 'APELLIDO' in colonne:  # Noms\n",
    "            correction = \"Exemple\"\n",
    "        elif 'NUM' in colonne or 'CUENTA' in colonne:  # Nombres\n",
    "            correction = \"1234567890\"\n",
    "        else:\n",
    "            correction = \"VALEUR_DEFAUT\"\n",
    "\n",
    "        print(f\"Correction manuelle générée pour {colonne}: {correction}\")\n",
    "        return correction\n",
    "\n",
    "    def generate_corrections_batch(self, \n",
    "                                  batch_data: List[Dict[str, Any]], \n",
    "                                  regex_data: Dict[str, Dict[str, Any]],\n",
    "                                  system_prompt: str) -> List[Dict]:\n",
    "        # Formatage des données pour le prompt\n",
    "        formatted_batch = json.dumps(batch_data, ensure_ascii=False, indent=2)\n",
    "        formatted_regex = json.dumps(regex_data, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        user_content = f\"\"\"\n",
    "Je dois corriger des erreurs dans un fichier d'assurance santé.\n",
    "\n",
    "Voici les erreurs à corriger (le format est déjà défini, j'ai juste besoin de valeurs pour 'correction'):\n",
    "{formatted_batch}\n",
    "\n",
    "Voici les informations sur les formats attendus pour chaque colonne:\n",
    "{formatted_regex}\n",
    "\n",
    "Pour chaque ligne d'erreur, donnez-moi uniquement une valeur pour la colonne 'correction' qui:\n",
    "1. Respecte le format (regex) de la colonne concernée\n",
    "2. Est une donnée plausible pour un contexte d'assurance santé espagnol\n",
    "3. Est variée (pas toujours la même valeur pour un même type de colonne)\n",
    "\n",
    "Renvoyez exactement le même format JSON que celui que je vous ai fourni, mais avec la colonne 'correction' remplie.\n",
    "\"\"\"\n",
    "        \n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ]\n",
    "        \n",
    "        max_retries = 5\n",
    "        retry_delay = 15\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.chat(messages)\n",
    "                response_content = response['choices'][0]['message']['content']\n",
    "                \n",
    "                # Log pour débug\n",
    "                print(f\"Réponse brute de l'API (premiers 200 caractères): {response_content[:200]}...\")\n",
    "                \n",
    "                # Nettoyage de la réponse\n",
    "                clean_content = response_content.strip()\n",
    "                if clean_content.startswith(\"```json\"):\n",
    "                    clean_content = clean_content[7:]\n",
    "                if clean_content.endswith(\"```\"):\n",
    "                    clean_content = clean_content[:-3]\n",
    "                clean_content = clean_content.strip()\n",
    "                \n",
    "                try:\n",
    "                    corrected_batch = json.loads(clean_content)\n",
    "                    \n",
    "                    # Vérifier que la réponse est valide\n",
    "                    if not isinstance(corrected_batch, list):\n",
    "                        raise ValueError(\"La réponse n'est pas une liste valide\")\n",
    "                    \n",
    "                    for item in corrected_batch:\n",
    "                        if not isinstance(item, dict) or 'correction' not in item:\n",
    "                            raise ValueError(\"Un élément de la réponse est invalide ou manque la clé 'correction'\")\n",
    "                        item['cost_metrics'] = response.get('cost_metrics', {})\n",
    "                    \n",
    "                    return corrected_batch\n",
    "                    \n",
    "                except json.JSONDecodeERROR as e:\n",
    "                    print(f\"Erreur de décodage JSON: {str(e)}\")\n",
    "                    print(f\"Contenu nettoyé: {clean_content[:200]}...\")\n",
    "                    raise\n",
    "            \n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    print(f\"Erreur lors de la génération des corrections (tentative {attempt+1}/{max_retries}): {str(e)}\")\n",
    "                    print(f\"Nouvelle tentative dans {retry_delay} secondes...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"Échec de la génération après {max_retries} tentatives: {str(e)}\")\n",
    "                    # En cas d'échec après toutes les tentatives, générer manuellement\n",
    "                    for item in batch_data:\n",
    "                        colonne = item['Columna']\n",
    "                        regex_info = regex_data.get(colonne, {})\n",
    "                        item['correction'] = self.generer_correction_manuelle(item, regex_info)\n",
    "                        item['cost_metrics'] = {'ERROR': str(e)}\n",
    "                    return batch_data\n",
    "\n",
    "def lire_fichiers(fichier_excel, fichier_regex):\n",
    "    \"\"\"\n",
    "    Lit les onglets du fichier Excel et le fichier de regex\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lire l'onglet 'Macros Alta' et 'ERROR'\n",
    "        df_macros = pd.read_excel(fichier_excel, sheet_name='Macros Alta')\n",
    "        df_ERROR = pd.read_excel(fichier_excel, sheet_name='ERROR')\n",
    "        \n",
    "        # Vérifier les noms de colonnes dans df_ERROR\n",
    "        print(f\"Colonnes dans l'onglet 'ERROR': {list(df_ERROR.columns)}\")\n",
    "        \n",
    "        # S'assurer que les colonnes existent avec la bonne casse\n",
    "        if 'Columna' not in df_ERROR.columns:\n",
    "            if 'Columna' in df_ERROR.columns:\n",
    "                df_ERROR.rename(columns={'Columna': 'Columna'}, inplace=True)\n",
    "                print(\"Colonne 'Columna' renommée en 'Columna'\")\n",
    "            else:\n",
    "                print(\"Attention: Colonne 'Columna' introuvable\")\n",
    "                \n",
    "        if 'Linea' not in df_ERROR.columns:\n",
    "            if 'Linea' in df_ERROR.columns:\n",
    "                df_ERROR.rename(columns={'Linea': 'Linea'}, inplace=True)\n",
    "                print(\"Colonne 'Linea' renommée en 'Linea'\")\n",
    "            else:\n",
    "                print(\"Attention: Colonne 'Linea' introuvable\")\n",
    "        \n",
    "        print(f\"Onglet 'Macros Alta' chargé: {df_macros.shape[0]} lignes, {df_macros.shape[1]} colonnes\")\n",
    "        print(f\"Onglet 'ERROR' chargé: {df_ERROR.shape[0]} lignes\")\n",
    "        \n",
    "        # Lire le fichier regex amélioré\n",
    "        df_regex = pd.read_excel(fichier_regex)\n",
    "        print(f\"Fichier regex chargé: {df_regex.shape[0]} règles\")\n",
    "        \n",
    "        return df_macros, df_ERROR, df_regex\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erreur lors de la lecture des fichiers: {str(e)}\")\n",
    "\n",
    "def preparer_regex_dict(df_regex):\n",
    "    \"\"\"\n",
    "    Prépare un dictionnaire de regexes par colonne pour faciliter l'accès\n",
    "    \"\"\"\n",
    "    regex_dict = {}\n",
    "    for _, row in df_regex.iterrows():\n",
    "        column_name = row['column_name']\n",
    "        \n",
    "        regex_dict[column_name] = {\n",
    "            'regex_id': row['regex_id'] if 'regex_id' in row else None,\n",
    "            'regex_rule': row['regex_rule'] if 'regex_rule' in row else \"\",\n",
    "            'regex_description': row['regex_description'] if 'regex_description' in row else \"\",\n",
    "            'regex_rule_amelio': row['regex_rule_amelio'] if 'regex_rule_amelio' in row and pd.notna(row['regex_rule_amelio']) else \"\"\n",
    "        }\n",
    "    \n",
    "    return regex_dict\n",
    "\n",
    "def preparer_donnees_erreur(df_ERROR):\n",
    "    \"\"\"\n",
    "    Prépare les données d'erreur pour le LLM\n",
    "    \"\"\"\n",
    "    # S'assurer que la colonne 'correction' existe\n",
    "    if 'correction' not in df_ERROR.columns:\n",
    "        df_ERROR['correction'] = \"\"\n",
    "    \n",
    "    # Convertir en liste de dictionnaires pour faciliter le traitement\n",
    "    erreurs = []\n",
    "    for _, row in df_ERROR.iterrows():\n",
    "        erreur = {\n",
    "            'Columna': row['Columna'],\n",
    "            'Linea': row['Linea'],\n",
    "            'correction': \"\"\n",
    "        }\n",
    "        erreurs.append(erreur)\n",
    "    \n",
    "    return erreurs\n",
    "\n",
    "def generer_corrections_via_llm(erreurs, regex_dict, batch_size=3):\n",
    "    \"\"\"\n",
    "    Génère des corrections via le LLM pour toutes les erreurs\n",
    "    Réduit la taille des lots pour minimiser les erreurs\n",
    "    \"\"\"\n",
    "    # Initialiser le générateur SecureGPT\n",
    "    generator = SecureGPTCorrectionsGenerator(\n",
    "        token=get_auth_token(),\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Prompt système pour le LLM\n",
    "    SYSTEM_PROMPT = \"\"\"Vous êtes un expert en génération de données synthétiques pour des tests.\n",
    "Votre tâche est de générer des valeurs réalistes pour corriger des erreurs dans un fichier de données d'assurance santé en Espagne.\n",
    "\n",
    "Pour chaque ligne d'erreur que je vous donne, générez une valeur de correction qui:\n",
    "1. Respecte strictement le format (regex) de la colonne concernée\n",
    "2. Est une donnée synthétique mais plausible pour l'assurance santé\n",
    "3. Est variée (évitez les répétitions)\n",
    "\n",
    "Renvoyez uniquement un JSON valide avec le même format que l'entrée, mais avec la colonne \"correction\" remplie.\n",
    "N'ajoutez aucune explication ou texte supplémentaire en dehors du JSON.\"\"\"\n",
    "    \n",
    "    # Traiter par lots\n",
    "    resultats = []\n",
    "    total_cost = 0\n",
    "    \n",
    "    # Diviser en lots plus petits pour réduire les risques d'erreur\n",
    "    for i in tqdm(range(0, len(erreurs), batch_size), desc=\"Traitement des lots\"):\n",
    "        batch = erreurs[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Générer les corrections pour le lot\n",
    "            corrected_batch = generator.generate_corrections_batch(\n",
    "                batch, \n",
    "                regex_dict,\n",
    "                SYSTEM_PROMPT\n",
    "            )\n",
    "            \n",
    "            resultats.extend(corrected_batch)\n",
    "            \n",
    "            # Calculer le coût\n",
    "            for item in corrected_batch:\n",
    "                if 'cost_metrics' in item and 'total_cost' in item['cost_metrics']:\n",
    "                    total_cost += item['cost_metrics']['total_cost']\n",
    "            \n",
    "            # Pause plus longue entre les lots\n",
    "            time.sleep(2)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du lot {i//batch_size + 1}: {str(e)}\")\n",
    "            # En cas d'erreur, générer manuellement pour ce lot\n",
    "            for item in batch:\n",
    "                colonne = item['Columna']\n",
    "                regex_info = regex_dict.get(colonne, {})\n",
    "                item['correction'] = generator.generer_correction_manuelle(item, regex_info)\n",
    "                resultats.append(item)\n",
    "    \n",
    "    print(f\"Génération terminée. Coût total: {total_cost:.6f} $\")\n",
    "    return resultats\n",
    "\n",
    "def mettre_a_jour_df_ERROR(df_ERROR, corrections):\n",
    "    \"\"\"\n",
    "    Met à jour le DataFrame d'erreurs avec les corrections\n",
    "    \"\"\"\n",
    "    # Créer un dictionnaire pour faciliter la recherche\n",
    "    corrections_dict = {}\n",
    "    for correction in corrections:\n",
    "        # On utilise les noms de colonnes corrects (avec majuscules)\n",
    "        key = (correction['Columna'], correction['Linea'])\n",
    "        corrections_dict[key] = correction['correction']\n",
    "    \n",
    "    # Mettre à jour le DataFrame\n",
    "    for index, row in df_ERROR.iterrows():\n",
    "        key = (row['Columna'], row['Linea'])\n",
    "        if key in corrections_dict:\n",
    "            df_ERROR.at[index, 'correction'] = corrections_dict[key]\n",
    "    \n",
    "    return df_ERROR\n",
    "\n",
    "# Fonction principale\n",
    "def generer_corrections_llm(fichier_excel, fichier_regex, fichier_sortie):\n",
    "    \"\"\"\n",
    "    Fonction principale pour générer les corrections via LLM\n",
    "    \"\"\"\n",
    "    # Lire les fichiers\n",
    "    df_macros, df_ERROR, df_regex = lire_fichiers(fichier_excel, fichier_regex)\n",
    "    \n",
    "    # Préparer le dictionnaire de regex\n",
    "    regex_dict = preparer_regex_dict(df_regex)\n",
    "    \n",
    "    # Préparer les données d'erreur\n",
    "    erreurs = preparer_donnees_erreur(df_ERROR)\n",
    "    \n",
    "    # Générer les corrections via LLM\n",
    "    corrections = generer_corrections_via_llm(erreurs, regex_dict, batch_size=3)\n",
    "    \n",
    "    # Mettre à jour le DataFrame d'erreurs\n",
    "    df_ERROR_corrige = mettre_a_jour_df_ERROR(df_ERROR, corrections)\n",
    "    \n",
    "    # Sauvegarder le résultat\n",
    "    df_ERROR_corrige.to_excel(fichier_sortie, index=False)\n",
    "    print(f\"Corrections générées et sauvegardées dans {fichier_sortie}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Fichiers d'entrée/sortie\n",
    "    FICHIER_EXCEL = \"250331200000003_Health_Listado_asegurados_anonym.xlsx\"\n",
    "    FICHIER_REGEX = \"regex_improved_results.xlsx\"\n",
    "    FICHIER_SORTIE = \"ERROR_avec_corrections_llm.xlsx\"\n",
    "    \n",
    "    # Générer les corrections\n",
    "    generer_corrections_llm(FICHIER_EXCEL, FICHIER_REGEX, FICHIER_SORTIE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ed0a34-f544-48b4-8fb2-f18344ac5219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Etape2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9143b2a3-e76f-4799-b9de-856cb559c8a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes dans le fichier de corrections: ['Columna', 'Linea', 'Valor Incorrecto', 'Descripcion', 'Correction', 'correction']\nDonnées principales chargées: 22 lignes, 35 colonnes\nCorrections chargées: 113 corrections\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[0] = '4.0' -> '4'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[4] = '4.0' -> '3'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[8] = '3.0' -> '5'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[12] = '1.0' -> '3'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[13] = '1.0' -> '5'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[14] = '1.0' -> '2'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[15] = '1.0' -> '3'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[16] = '1.0' -> '4'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[17] = '1.0' -> '5'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[18] = '1.0' -> '3'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[19] = '1.0' -> '5'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[20] = '1.0' -> '4'\nCorrection appliquée: NUM_MIEMBROS_DE_LA_FAMILIA[21] = '1.0' -> '4'\nCorrection appliquée: CODIGO_BANCO[0] = '49' -> 'ES9121000418401234567890'\nCorrection appliquée: CODIGO_BANCO[1] = '49' -> 'ES7620850119341234567891'\nCorrection appliquée: CODIGO_BANCO[2] = '49' -> 'ES9121000418450200051332'\nCorrection appliquée: CODIGO_BANCO[3] = '49' -> 'ES7620800001183063001234'\nCorrection appliquée: CODIGO_BANCO[4] = '49' -> 'ES3500490001001000000000'\nCorrection appliquée: CODIGO_BANCO[5] = '49' -> 'ES9121000418450200051332'\nCorrection appliquée: CODIGO_BANCO[6] = '49' -> 'ES4620385779493020005048'\nCorrection appliquée: CODIGO_BANCO[7] = '49' -> 'ES7620770024003102575766'\nCorrection appliquée: CODIGO_BANCO[8] = '49' -> 'ES9121000418450200051332'\nCorrection appliquée: CODIGO_BANCO[9] = '50' -> 'ES7620770024003102575766'\nCorrection appliquée: CODIGO_BANCO[10] = '51' -> 'ES4821000761234567891234'\nCorrection appliquée: CODIGO_BANCO[11] = '49' -> 'ES9121000418450200051332'\nCorrection appliquée: CODIGO_BANCO[12] = '49' -> 'ES3020800601441234567890'\nCorrection appliquée: CODIGO_BANCO[13] = '49' -> 'ES4521001234567890123456'\nCorrection appliquée: CODIGO_BANCO[14] = '49' -> 'ES9121000418450200051332'\nCorrection appliquée: CODIGO_BANCO[15] = '49' -> 'ES7620770024003102575766'\nCorrection appliquée: CODIGO_BANCO[16] = '49' -> 'ES6000491500051234567892'\nCorrection appliquée: CODIGO_BANCO[17] = '49' -> 'ES9121000418450200051332'\nCorrection appliquée: CODIGO_BANCO[18] = '49' -> 'ES7620770024003102575766'\nCorrection appliquée: CODIGO_BANCO[19] = '49' -> 'ES6000491500051234567892'\nCorrection appliquée: CODIGO_BANCO[20] = '49' -> 'ES9121000418450200051332'\nCorrection appliquée: CODIGO_BANCO[21] = '50' -> 'ES1420805804111234567890'\nCorrection appliquée: DC[0] = '22' -> '4'\nCorrection appliquée: DC[1] = '22' -> '3'\nCorrection appliquée: DC[2] = '22' -> '7'\nCorrection appliquée: DC[3] = '22' -> '9'\nCorrection appliquée: DC[4] = '22' -> '3'\nCorrection appliquée: DC[5] = '22' -> '7'\nCorrection appliquée: DC[6] = '22' -> '5'\nCorrection appliquée: DC[7] = '22' -> '4'\nCorrection appliquée: DC[8] = '22' -> '7'\nCorrection appliquée: DC[9] = '22' -> '3'\nCorrection appliquée: DC[10] = '22' -> '3'\nCorrection appliquée: DC[11] = '22' -> '7'\nCorrection appliquée: DC[12] = '22' -> '9'\nCorrection appliquée: DC[13] = '22' -> '5'\nCorrection appliquée: DC[14] = '22' -> '3'\nCorrection appliquée: DC[15] = '22' -> '7'\nCorrection appliquée: DC[16] = '22' -> '5'\nCorrection appliquée: DC[17] = '22' -> '3'\nCorrection appliquée: DC[18] = '22' -> '7'\nCorrection appliquée: DC[19] = '22' -> '3'\nCorrection appliquée: DC[20] = '22' -> '7'\nCorrection appliquée: DC[21] = '22' -> '1'\nCorrection appliquée: NIF_CIF_ASEGURADO[4] = 'Y1111111V' -> 'X1234567L'\nCorrection appliquée: PRIMER_APELLIDO[5] = 'Appellido' -> 'González'\nCorrection appliquée: SEGUNDO_APELLIDO[4] = 'GASCUEÑA' -> 'Rodríguez'\nCorrection appliquée: SEGUNDO_APELLIDO[17] = 'MARTINEZ' -> 'González'\nCorrection appliquée: SEGUNDO_APELLIDO[20] = 'MARTINEZ' -> 'Fernández'\nCorrection appliquée: NOMBRE[1] = 'JoHN' -> 'María'\nCorrection appliquée: ESTADO_CIVIL[1] = 'Casada' -> 'soltero'\nCorrection appliquée: ESTADO_CIVIL[2] = 'Soltera' -> 'casado'\nCorrection appliquée: ESTADO_CIVIL[3] = 'Soltera' -> 'divorciado'\nCorrection appliquée: ESTADO_CIVIL[5] = 'Casada' -> 'Casado'\nCorrection appliquée: ESTADO_CIVIL[6] = 'Soltera' -> 'Viudo'\nCorrection appliquée: ESTADO_CIVIL[7] = 'Soltera' -> 'Divorciado'\nCorrection appliquée: ESTADO_CIVIL[8] = 'Casada' -> 'soltero'\nCorrection appliquée: ESTADO_CIVIL[10] = 'Soltera' -> 'casado'\nCorrection appliquée: ESTADO_CIVIL[11] = 'Soltera' -> 'viudo'\nCorrection appliquée: ESTADO_CIVIL[12] = 'Soltera' -> 'soltero'\nCorrection appliquée: ESTADO_CIVIL[13] = 'Soltera' -> 'casado'\nCorrection appliquée: ESTADO_CIVIL[14] = 'Soltera' -> 'viudo'\nCorrection appliquée: ESTADO_CIVIL[17] = 'Soltera' -> 'Casado'\nCorrection appliquée: ESTADO_CIVIL[18] = 'Casada' -> 'Soltero'\nCorrection appliquée: ESTADO_CIVIL[21] = 'Soltera' -> 'Divorciado'\nCorrection appliquée: CODIGO_POSTAL[0] = '8029' -> '28001'\nCorrection appliquée: CODIGO_POSTAL[1] = '8029' -> '08015'\nCorrection appliquée: CODIGO_POSTAL[2] = '8029' -> '46007'\nCorrection appliquée: CODIGO_POSTAL[3] = '8029' -> '28013'\nCorrection appliquée: CODIGO_POSTAL[13] = '8912' -> '08001'\nCorrection appliquée: CODIGO_POSTAL[16] = '8028' -> '46007'\nCorrection appliquée: CODIGO_POSTAL[18] = '9001' -> '28001'\nCorrection appliquée: CODIGO_POSTAL[19] = '8024' -> '08002'\nCorrection appliquée: POBLACION[4] = 'STA. CRUZ DE T.' -> 'Valencia'\nCorrection appliquée: POBLACION[5] = 'STA. CRUZ DE T.' -> 'Madrid'\nCorrection appliquée: POBLACION[6] = 'STA. CRUZ DE T.' -> 'Barcelona'\nCorrection appliquée: POBLACION[7] = 'STA. CRUZ DE T.' -> 'Valencia'\nCorrection appliquée: FECHA_EFECTO[0] = '2025-02-01 00:00:00' -> '2023-09-15'\nCorrection appliquée: FECHA_EFECTO[1] = '2025-02-01 00:00:00' -> '2024-01-01'\nCorrection appliquée: FECHA_EFECTO[2] = '2025-02-01 00:00:00' -> '2023-03-23'\nCorrection appliquée: FECHA_EFECTO[3] = '2025-02-01 00:00:00' -> '2023-07-15'\nCorrection appliquée: FECHA_EFECTO[4] = '2025-02-01 00:00:00' -> '2023-09-01'\nCorrection appliquée: FECHA_EFECTO[5] = '2025-02-01 00:00:00' -> '2023-11-23'\nCorrection appliquée: FECHA_EFECTO[6] = '2025-02-01 00:00:00' -> '2023-03-15'\nCorrection appliquée: FECHA_EFECTO[7] = '2025-02-01 00:00:00' -> '2022-01-01'\nCorrection appliquée: FECHA_EFECTO[8] = '2025-02-01 00:00:00' -> '2021-06-30'\nCorrection appliquée: FECHA_EFECTO[9] = '2025-02-01 00:00:00' -> '2023-03-15'\nCorrection appliquée: FECHA_EFECTO[10] = '2025-02-01 00:00:00' -> '2023-07-28'\nCorrection appliquée: FECHA_EFECTO[11] = '2025-02-01 00:00:00' -> '2023-11-02'\nCorrection appliquée: FECHA_EFECTO[12] = '2025-02-01 00:00:00' -> '2023-03-15'\nCorrection appliquée: FECHA_EFECTO[13] = '2025-02-01 00:00:00' -> '2022-12-01'\nCorrection appliquée: FECHA_EFECTO[14] = '2025-02-01 00:00:00' -> '2021-06-27'\nCorrection appliquée: FECHA_EFECTO[15] = '2025-02-01 00:00:00' -> '2023-03-15'\nCorrection appliquée: FECHA_EFECTO[16] = '2025-02-01 00:00:00' -> '2022-07-28'\nCorrection appliquée: FECHA_EFECTO[17] = '2025-02-01 00:00:00' -> '2021-11-06'\nCorrection appliquée: FECHA_EFECTO[18] = '2025-02-01 00:00:00' -> '2023-03-15'\nCorrection appliquée: FECHA_EFECTO[19] = '2025-02-01 00:00:00' -> '2022-11-01'\nCorrection appliquée: FECHA_EFECTO[20] = '2025-02-01 00:00:00' -> '2021-07-28'\nCorrection appliquée: FECHA_EFECTO[21] = '2025-02-01 00:00:00' -> '2023-04-15'\nCorrection appliquée: RAZON_SOCIAL[0] = 'COUTOT ROEHRIG' -> 'Seguros Vida S.A.'\n\nRésumé: 113 corrections appliquées, 0 corrections ignorées\n\nDonnées corrigées sauvegardées dans 250331200000003_Health_Listado_asegurados_corrige.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def lire_fichiers(fichier_data, fichier_corrections):\n",
    "    try:\n",
    "        # Lire l'onglet 'Macros Alta' du fichier principal\n",
    "        df_macros = pd.read_excel(fichier_data, sheet_name='Macros Alta')\n",
    "        \n",
    "        # Lire le fichier de corrections\n",
    "        df_corrections = pd.read_excel(fichier_corrections)\n",
    "        \n",
    "        print(f\"Colonnes dans le fichier de corrections: {list(df_corrections.columns)}\")\n",
    "        \n",
    "        # S'assurer que les colonnes nécessaires existent avec la bonne casse\n",
    "        if 'Columna' not in df_corrections.columns and 'columna' in df_corrections.columns:\n",
    "            df_corrections.rename(columns={'columna': 'Columna'}, inplace=True)\n",
    "            print(\"Colonne 'columna' renommée en 'Columna'\")\n",
    "            \n",
    "        if 'Linea' not in df_corrections.columns and 'linea' in df_corrections.columns:\n",
    "            df_corrections.rename(columns={'linea': 'Linea'}, inplace=True)\n",
    "            print(\"Colonne 'linea' renommée en 'Linea'\")\n",
    "        \n",
    "        print(f\"Données principales chargées: {df_macros.shape[0]} lignes, {df_macros.shape[1]} colonnes\")\n",
    "        print(f\"Corrections chargées: {df_corrections.shape[0]} corrections\")\n",
    "        \n",
    "        return df_macros, df_corrections\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Erreur lors de la lecture des fichiers: {str(e)}\")\n",
    "\n",
    "def appliquer_corrections(df_macros, df_corrections):\n",
    "    # Vérifier que les colonnes nécessaires existent\n",
    "    colonnes_requises = ['Columna', 'Linea', 'correction']\n",
    "    for col in colonnes_requises:\n",
    "        if col not in df_corrections.columns:\n",
    "            raise ValueError(f\"La colonne '{col}' est manquante dans le fichier de corrections\")\n",
    "    \n",
    "    corrections_appliquees = 0\n",
    "    corrections_ignorees = 0\n",
    "    \n",
    "    for idx, correction in df_corrections.iterrows():\n",
    "        try:\n",
    "            colonne = correction['Columna']\n",
    "            \n",
    "            # Conversion en entier avec gestion des erreurs\n",
    "            try:\n",
    "                ligne = int(float(correction['Linea']))\n",
    "            except:\n",
    "                print(f\"Erreur de conversion pour ligne {correction['Linea']} (type: {type(correction['Linea'])})\")\n",
    "                ligne = -1\n",
    "            \n",
    "            nouvelle_valeur = correction['correction']\n",
    "            \n",
    "            if pd.isna(nouvelle_valeur) or nouvelle_valeur == \"\":\n",
    "                print(f\"Ignoré: Valeur de correction vide pour {colonne}[{ligne}]\")\n",
    "                corrections_ignorees += 1\n",
    "                continue\n",
    "            \n",
    "            # Vérifier que la colonne existe\n",
    "            if colonne not in df_macros.columns:\n",
    "                print(f\"Avertissement: Colonne '{colonne}' non trouvée dans les données principales\")\n",
    "                corrections_ignorees += 1\n",
    "                continue\n",
    "            \n",
    "            # Vérifier que la ligne existe\n",
    "            if ligne < 0 or ligne >= len(df_macros):\n",
    "                print(f\"Avertissement: Ligne {ligne} hors limites (0-{len(df_macros)-1})\")\n",
    "                corrections_ignorees += 1\n",
    "                continue\n",
    "                \n",
    "            # Sauvegarder l'ancienne valeur pour le log\n",
    "            ancienne_valeur = df_macros.at[ligne, colonne]\n",
    "            \n",
    "            # Traitement spécial pour les dates avec format DD/MM/YYYY\n",
    "            if 'FECHA' in colonne and isinstance(nouvelle_valeur, str) and '/' in nouvelle_valeur:\n",
    "                try:\n",
    "                    # Convertir explicitement la date au format correct\n",
    "                    date_parts = nouvelle_valeur.split('/')\n",
    "                    if len(date_parts) == 3 and len(date_parts[0]) <= 2 and len(date_parts[1]) <= 2:\n",
    "                        # C'est probablement une date DD/MM/YYYY\n",
    "                        jour, mois, annee = map(int, date_parts)\n",
    "                        nouvelle_valeur = f\"{annee}-{mois:02d}-{jour:02d}\"\n",
    "                except:\n",
    "                    # Si la conversion échoue, garder la valeur originale\n",
    "                    pass\n",
    "            \n",
    "            # Appliquer la correction\n",
    "            df_macros.at[ligne, colonne] = nouvelle_valeur\n",
    "            \n",
    "            print(f\"Correction appliquée: {colonne}[{ligne}] = '{ancienne_valeur}' -> '{nouvelle_valeur}'\")\n",
    "            corrections_appliquees += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de l'application de la correction (index {idx}): {str(e)}\")\n",
    "            corrections_ignorees += 1\n",
    "    \n",
    "    print(f\"\\nRésumé: {corrections_appliquees} corrections appliquées, {corrections_ignorees} corrections ignorées\")\n",
    "    return df_macros\n",
    "\n",
    "def appliquer_corrections_excel(fichier_data, fichier_corrections, fichier_sortie):\n",
    "    # Charger les données\n",
    "    df_macros, df_corrections = lire_fichiers(fichier_data, fichier_corrections)\n",
    "    \n",
    "    # Appliquer les corrections\n",
    "    df_macros_corrige = appliquer_corrections(df_macros, df_corrections)\n",
    "    \n",
    "    # Sauvegarder le résultat\n",
    "    with pd.ExcelWriter(fichier_sortie) as writer:\n",
    "        df_macros_corrige.to_excel(writer, sheet_name='macros alta', index=False)\n",
    "        # Conserver l'onglet 'ERROR' original\n",
    "        pd.read_excel(fichier_data, sheet_name='ERROR').to_excel(writer, sheet_name='ERROR', index=False)\n",
    "    \n",
    "    print(f\"\\nDonnées corrigées sauvegardées dans {fichier_sortie}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Fichiers d'entrée/sortie\n",
    "    FICHIER_DATA = \"250331200000003_Health_Listado_asegurados_anonym.xlsx\"\n",
    "    FICHIER_CORRECTIONS = \"ERROR_avec_corrections_llm.xlsx\"\n",
    "    FICHIER_SORTIE = \"250331200000003_Health_Listado_asegurados_corrige.xlsx\"\n",
    "    \n",
    "    # Appliquer les corrections\n",
    "    appliquer_corrections_excel(FICHIER_DATA, FICHIER_CORRECTIONS, FICHIER_SORTIE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2866975-58da-463f-9f0b-ccebe205dfcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Simple Regex Transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}